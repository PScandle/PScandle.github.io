<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN">
<html>
<head>
    <title>CTIS A3 - Technology</title>
    <link rel="stylesheet" href="../styles/styles.css">
    <img src = "../images/main_logo.png">
</head>

<body>
<!-- Site navigation menu -->
<ul class="navigator">
    <li><a href="../index.html">Home page</a>
    <li><a href="Technology.html">Technology/Topic</a>
    <li><a href="Opportunities.html">Opportunities</a>
    <li><a href="Choices.html">Choices</a>
    <li><a href="Risks.html">Risks</a>
    <li><a href="Reflection.html">Reflection</a>
    <li><a href="References.html"> References </a>
</ul>
<!-- Main content -->
<h1>Risks Presented</h1>

<br>
<p>
    The first of these, the risk of failure or unforeseen error, is higher when the algorithm is first adopted, as this
    is the time where there will be the least data available and thus the largest scope for errors. If an AI system
    were to recommend the incorrect drug for a patient, miss a tumour on a radiological scan or incorrectly allocate
    a hospital bed for a patient when another needed it more injury or death could result (II, 2019). While many
    errors like this exist in medicine today, the repercussions could be different if these were due to a software
    error rather than simply human error. Furthermore, if one system were to be used across multiple clinics, one
    error could cause tens of thousands of injuries, rather than just a few if it were human error.

    <br>
    <br>

    Any implicit bias gained due to systemic bias in data collection within clinics can affect the patterns recognised
    by the AI (Hashimoto, Rosman, Rus, & Meireles, 2018). This is something that researchers and practitioners must be
    closely aware of, and as such they should work with the data scientists developing the models to limit this as
    much as possible (II, 2019). Issues could arise if patients decide to leave one treatment provider, switching to
    another midway through treatment, leaving a gap in the datasets of both clinics. This creates fragmentation, both
    decreasing the comprehensiveness of the dataset as well as the risk of error. This also increases the cost of
    gathering data, creating a higher barrier to entry to creating effective health care AI (II, 2019).

    <br>
    <br>

    Alongside the issue with inherent bias if clinical data collection is done improperly, there is another potential
    risk of bias – insufficient training data. As AI systems learn from the data on which they are trained, if this is
    performed improperly inherent bias accrued from the training data can be incorporated into their data (II, 2019).
    For instance, if a system was trained off data gained from one population only, then used in an area with a
    completely different population, any issues or diseases that are more common in the new area may not be treated
    appropriately, as the system may not have adequate experience with these differing issues.

    <br>
    <br>

    The final risk of note in this report involves the privacy of sensitive medical data. As AI systems require large
    datasets to train effectively, developers are incentivised to conduct large-scale datasets from patients. There is
    a concern that this data collection may infringe on patient privacy, as medical data is sensitive and highly
    personal. Additionally, there is a risk that AI systems could predict or reveal highly personal outcomes for
    patients, even if this is not something that was disclosed to the system. An example of this could involve
    recognising that a person may have Parkinson’s disease through the shaking of a computer mouse. Some patients may
    consider this a privacy violation, especially if the AI’s inference was available to third parties (II, 2019).
</p>

<!-- Sign and date the page, it's only polite! -->
<address>Created 10 June 2022<br>
    by Peter Scandle.</address>

</body>
</html>
